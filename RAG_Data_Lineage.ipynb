{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function dotenv.main.load_dotenv(dotenv_path: Union[str, ForwardRef('os.PathLike[str]'), NoneType] = None, stream: Optional[IO[str]] = None, verbose: bool = False, override: bool = False, interpolate: bool = True, encoding: Optional[str] = 'utf-8') -> bool>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LLM to be used\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required LangChain modules\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 - Document Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read the list of PDFs in the dir\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "import os\n",
    "\n",
    "pdfs = []\n",
    "for root, dirs, files in os.walk(\"Scripts\"):\n",
    "    # print(root, dirs, files)\n",
    "    for file in files:\n",
    "        if file.endswith(\".pdf\"):\n",
    "            pdfs.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for pdf in pdfs:\n",
    "    loader = PyMuPDFLoader(pdf)\n",
    "    temp = loader.load()\n",
    "    docs.extend(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-23T17:24:55+05:30', 'source': 'Scripts\\\\Dummy_Script.pdf', 'file_path': 'Scripts\\\\Dummy_Script.pdf', 'total_pages': 2, 'format': 'PDF 1.7', 'title': '', 'author': 'Amit Bhatia', 'subject': '', 'keywords': '', 'moddate': '2025-09-23T17:24:55+05:30', 'trapped': '', 'modDate': \"D:20250923172455+05'30'\", 'creationDate': \"D:20250923172455+05'30'\", 'page': 0}, page_content=\"/* Define a library for raw data if not already defined */ \\nlibname rawdata '/sas/data/raw'; \\n \\n/* Define a library for processed data if not already defined */ \\nlibname processed '/sas/data/processed'; \\n \\n/* Step 1: Load raw customer orders data */ \\ndata processed.customer_orders_raw; \\n    set rawdata.orders; \\n    /* Simulate adding a new derived variable */ \\n    OrderValue = Quantity * UnitPrice; \\nrun; \\n \\n/* Step 2: Filter orders for a specific region */ \\ndata processed.customer_orders_filtered; \\n    set processed.customer_orders_raw; \\n    where Region = 'East'; \\nrun; \\n \\n/* Step 3: Aggregate sales by customer */ \\nproc sql; \\n    create table processed.customer_sales_summary as \\n    select \\n        CustomerID, \\n        sum(OrderValue) as TotalSales, \\n        count(distinct OrderID) as NumberOfOrders \\n    from processed.customer_orders_filtered \\n    group by CustomerID; \\nquit; \\n \\n/* Step 4: Merge aggregated sales with customer demographic information */\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-23T17:24:55+05:30', 'source': 'Scripts\\\\Dummy_Script.pdf', 'file_path': 'Scripts\\\\Dummy_Script.pdf', 'total_pages': 2, 'format': 'PDF 1.7', 'title': '', 'author': 'Amit Bhatia', 'subject': '', 'keywords': '', 'moddate': '2025-09-23T17:24:55+05:30', 'trapped': '', 'modDate': \"D:20250923172455+05'30'\", 'creationDate': \"D:20250923172455+05'30'\", 'page': 1}, page_content='data processed.final_customer_data; \\n    merge processed.customer_sales_summary (in=a) \\n          rawdata.customer_demographics (in=b); \\n    by CustomerID; \\n    if a and b; /* Ensure records exist in both datasets */ \\nrun;')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error in __cdecl faiss::FileIOReader::FileIOReader(const char *) at D:\\a\\faiss-wheels\\faiss-wheels\\faiss\\faiss\\impl\\io.cpp:68: Error: 'f' failed: could not open C:\\Users\\91852\\Documents\\Machine Learning\\Gen AI\\AI_Agents\\RAG_Data_Lineage\\Scripts\\index.faiss for reading: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_44980\\2815312569.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdb_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr\"C:\\Users\\91852\\Documents\\Machine Learning\\Gen AI\\AI_Agents\\RAG_Data_Lineage\\Scripts\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mvector_store\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_local\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdb_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_dangerous_deserialization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#db_name = r\"C:\\Users\\91852\\Documents\\Machine Learning\\Gen AI\\Ollama_Udemy\\9.Vector_Stores\\health_supplements\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#vector_store = FAISS.load_local(db_name, embeddings, allow_dangerous_deserialization=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\91852\\Documents\\Machine Learning\\Gen AI\\AI_Agents\\env_langchain\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(cls, folder_path, embeddings, index_name, allow_dangerous_deserialization, **kwargs)\u001b[0m\n\u001b[0;32m   1201\u001b[0m             )\n\u001b[0;32m   1202\u001b[0m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1203\u001b[0m         \u001b[1;31m# load index separately since it is not picklable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1204\u001b[0m         \u001b[0mfaiss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdependable_faiss_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1205\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34mf\"{index_name}.faiss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1207\u001b[0m         \u001b[1;31m# load docstore and index_to_docstore_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1208\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34mf\"{index_name}.pkl\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\91852\\Documents\\Machine Learning\\Gen AI\\AI_Agents\\env_langchain\\lib\\site-packages\\faiss\\swigfaiss.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   9794\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 9795\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_swigfaiss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Error in __cdecl faiss::FileIOReader::FileIOReader(const char *) at D:\\a\\faiss-wheels\\faiss-wheels\\faiss\\faiss\\impl\\io.cpp:68: Error: 'f' failed: could not open C:\\Users\\91852\\Documents\\Machine Learning\\Gen AI\\AI_Agents\\RAG_Data_Lineage\\Scripts\\index.faiss for reading: No such file or directory"
     ]
    }
   ],
   "source": [
    "db_name = r\"C:\\Users\\91852\\Documents\\Machine Learning\\Gen AI\\AI_Agents\\RAG_Data_Lineage\\Scripts\"\n",
    "vector_store = FAISS.load_local(db_name, embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "#db_name = r\"C:\\Users\\91852\\Documents\\Machine Learning\\Gen AI\\Ollama_Udemy\\9.Vector_Stores\\health_supplements\"\n",
    "#vector_store = FAISS.load_local(db_name, embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1b - Document Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap = 200)\n",
    "chunks = splitter.create_documents([transcript])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content=\"i mean all machines do that to some extent they all enhance our natural capabilities obviously cars make us allow us to move faster than we can run but this was a machine to extend the mind and and then of course ai is the ultimate expression of what a machine may be able to do or learn so very naturally for me that thought extended into into ai quite quickly remember the the programming language that was first started special to the machine no it was just the base it was just i think it was just basic uh on the zx spectrum i don't know what specific form it was and then later on i got a commodore amiga which uh was a fantastic machine no you're just showing off so yeah well lots of my friends had atari st's and i i managed to get amigas it was a bit more powerful and uh and that was incredible and used to do um programming in assembler and and uh also amos basic this this specific form of basic it was incredible actually as well all my coding skills and when did you fall in love with\")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1c & 1d - Indexing (Embedding Generation and Storing in Vector Store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91852\\Documents\\Machine Learning\\Gen AI\\AI_Agents\\env_langchain\\lib\\site-packages\\langchain_core\\vectorstores\\base.py:848\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[0;32m    846\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ids\n\u001b[1;32m--> 848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\91852\\Documents\\Machine Learning\\Gen AI\\AI_Agents\\env_langchain\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1044\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m \n\u001b[0;32m   1027\u001b[0m \u001b[38;5;124;03mThis is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;124;03m        faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39membed_documents(texts)\n\u001b[1;32m-> 1044\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[0;32m   1045\u001b[0m     texts,\n\u001b[0;32m   1046\u001b[0m     embeddings,\n\u001b[0;32m   1047\u001b[0m     embedding,\n\u001b[0;32m   1048\u001b[0m     metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[0;32m   1049\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[0;32m   1050\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1051\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\91852\\Documents\\Machine Learning\\Gen AI\\AI_Agents\\env_langchain\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1001\u001b[0m, in \u001b[0;36mFAISS.__from\u001b[1;34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, distance_strategy, **kwargs)\u001b[0m\n\u001b[0;32m    998\u001b[0m     index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatIP(\u001b[38;5;28mlen\u001b[39m(embeddings[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m    999\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1000\u001b[0m     \u001b[38;5;66;03m# Default to L2, currently other metric types not initialized.\u001b[39;00m\n\u001b[1;32m-> 1001\u001b[0m     index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatL2(\u001b[38;5;28mlen\u001b[39m(\u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m))\n\u001b[0;32m   1002\u001b[0m docstore \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocstore\u001b[39m\u001b[38;5;124m\"\u001b[39m, InMemoryDocstore())\n\u001b[0;32m   1003\u001b[0m index_to_docstore_id \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex_to_docstore_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "vector_store = FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 - Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type = 'similarity', search_kwargs={'k':4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001FEBC66ACE0>, search_kwargs={'k': 4})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 - Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "      You are a SAS code reviewer focussing on Data Lineage.\n",
    "      Answer ONLY from the provided transcript context.\n",
    "      Give your answer in 2 -3 sentences.\n",
    "      If the context is insufficient, just say you don't know.\n",
    "\n",
    "      {context}\n",
    "      Question: {question}\n",
    "    \"\"\",\n",
    "    input_variables = ['context', 'question']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "question          = \"What is the source table?\"\n",
    "retrieved_docs    = retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='2b3f73dc-95a1-4cad-8d69-06ffdfcd2682', metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-23T17:24:55+05:30', 'source': 'Scripts\\\\Dummy_Script.pdf', 'file_path': 'Scripts\\\\Dummy_Script.pdf', 'total_pages': 2, 'format': 'PDF 1.7', 'title': '', 'author': 'Amit Bhatia', 'subject': '', 'keywords': '', 'moddate': '2025-09-23T17:24:55+05:30', 'trapped': '', 'modDate': \"D:20250923172455+05'30'\", 'creationDate': \"D:20250923172455+05'30'\", 'page': 0}, page_content=\"/* Define a library for raw data if not already defined */ \\nlibname rawdata '/sas/data/raw'; \\n \\n/* Define a library for processed data if not already defined */ \\nlibname processed '/sas/data/processed'; \\n \\n/* Step 1: Load raw customer orders data */ \\ndata processed.customer_orders_raw; \\n    set rawdata.orders; \\n    /* Simulate adding a new derived variable */ \\n    OrderValue = Quantity * UnitPrice; \\nrun; \\n \\n/* Step 2: Filter orders for a specific region */ \\ndata processed.customer_orders_filtered; \\n    set processed.customer_orders_raw; \\n    where Region = 'East'; \\nrun; \\n \\n/* Step 3: Aggregate sales by customer */ \\nproc sql; \\n    create table processed.customer_sales_summary as \\n    select \\n        CustomerID, \\n        sum(OrderValue) as TotalSales, \\n        count(distinct OrderID) as NumberOfOrders \\n    from processed.customer_orders_filtered \\n    group by CustomerID; \\nquit; \\n \\n/* Step 4: Merge aggregated sales with customer demographic information */\"),\n",
       " Document(id='a4d6da47-fb38-4424-88c0-e905947b6c2a', metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2025-09-23T17:24:55+05:30', 'source': 'Scripts\\\\Dummy_Script.pdf', 'file_path': 'Scripts\\\\Dummy_Script.pdf', 'total_pages': 2, 'format': 'PDF 1.7', 'title': '', 'author': 'Amit Bhatia', 'subject': '', 'keywords': '', 'moddate': '2025-09-23T17:24:55+05:30', 'trapped': '', 'modDate': \"D:20250923172455+05'30'\", 'creationDate': \"D:20250923172455+05'30'\", 'page': 1}, page_content='data processed.final_customer_data; \\n    merge processed.customer_sales_summary (in=a) \\n          rawdata.customer_demographics (in=b); \\n    by CustomerID; \\n    if a and b; /* Ensure records exist in both datasets */ \\nrun;')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/* Define a library for raw data if not already defined */ \\nlibname rawdata '/sas/data/raw'; \\n \\n/* Define a library for processed data if not already defined */ \\nlibname processed '/sas/data/processed'; \\n \\n/* Step 1: Load raw customer orders data */ \\ndata processed.customer_orders_raw; \\n    set rawdata.orders; \\n    /* Simulate adding a new derived variable */ \\n    OrderValue = Quantity * UnitPrice; \\nrun; \\n \\n/* Step 2: Filter orders for a specific region */ \\ndata processed.customer_orders_filtered; \\n    set processed.customer_orders_raw; \\n    where Region = 'East'; \\nrun; \\n \\n/* Step 3: Aggregate sales by customer */ \\nproc sql; \\n    create table processed.customer_sales_summary as \\n    select \\n        CustomerID, \\n        sum(OrderValue) as TotalSales, \\n        count(distinct OrderID) as NumberOfOrders \\n    from processed.customer_orders_filtered \\n    group by CustomerID; \\nquit; \\n \\n/* Step 4: Merge aggregated sales with customer demographic information */\\n\\ndata processed.final_customer_data; \\n    merge processed.customer_sales_summary (in=a) \\n          rawdata.customer_demographics (in=b); \\n    by CustomerID; \\n    if a and b; /* Ensure records exist in both datasets */ \\nrun;\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "context_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = prompt.invoke({\"context\": context_text, \"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text=\"\\n      You are a SAS code reviewer focussing on Data Lineage.\\n      Answer ONLY from the provided transcript context.\\n      Give your answer in 2 -3 sentences.\\n      If the context is insufficient, just say you don't know.\\n\\n      /* Define a library for raw data if not already defined */ \\nlibname rawdata '/sas/data/raw'; \\n \\n/* Define a library for processed data if not already defined */ \\nlibname processed '/sas/data/processed'; \\n \\n/* Step 1: Load raw customer orders data */ \\ndata processed.customer_orders_raw; \\n    set rawdata.orders; \\n    /* Simulate adding a new derived variable */ \\n    OrderValue = Quantity * UnitPrice; \\nrun; \\n \\n/* Step 2: Filter orders for a specific region */ \\ndata processed.customer_orders_filtered; \\n    set processed.customer_orders_raw; \\n    where Region = 'East'; \\nrun; \\n \\n/* Step 3: Aggregate sales by customer */ \\nproc sql; \\n    create table processed.customer_sales_summary as \\n    select \\n        CustomerID, \\n        sum(OrderValue) as TotalSales, \\n        count(distinct OrderID) as NumberOfOrders \\n    from processed.customer_orders_filtered \\n    group by CustomerID; \\nquit; \\n \\n/* Step 4: Merge aggregated sales with customer demographic information */\\n\\ndata processed.final_customer_data; \\n    merge processed.customer_sales_summary (in=a) \\n          rawdata.customer_demographics (in=b); \\n    by CustomerID; \\n    if a and b; /* Ensure records exist in both datasets */ \\nrun;\\n      Question: What is the source table?\\n    \")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided code, there are two primary source tables from the `rawdata` library. The `rawdata.orders` table is used to create the initial customer orders dataset, and the `rawdata.customer_demographics` table is merged in the final step.\n"
     ]
    }
   ],
   "source": [
    "answer = llm.invoke(final_prompt)\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(retrieved_docs):\n",
    "    context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    return context_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_chain = RunnableParallel({\n",
    "    'context': retriever | RunnableLambda(format_docs),\n",
    "    'question': RunnablePassthrough()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': \"/* Define a library for raw data if not already defined */ \\nlibname rawdata '/sas/data/raw'; \\n \\n/* Define a library for processed data if not already defined */ \\nlibname processed '/sas/data/processed'; \\n \\n/* Step 1: Load raw customer orders data */ \\ndata processed.customer_orders_raw; \\n    set rawdata.orders; \\n    /* Simulate adding a new derived variable */ \\n    OrderValue = Quantity * UnitPrice; \\nrun; \\n \\n/* Step 2: Filter orders for a specific region */ \\ndata processed.customer_orders_filtered; \\n    set processed.customer_orders_raw; \\n    where Region = 'East'; \\nrun; \\n \\n/* Step 3: Aggregate sales by customer */ \\nproc sql; \\n    create table processed.customer_sales_summary as \\n    select \\n        CustomerID, \\n        sum(OrderValue) as TotalSales, \\n        count(distinct OrderID) as NumberOfOrders \\n    from processed.customer_orders_filtered \\n    group by CustomerID; \\nquit; \\n \\n/* Step 4: Merge aggregated sales with customer demographic information */\\n\\ndata processed.final_customer_data; \\n    merge processed.customer_sales_summary (in=a) \\n          rawdata.customer_demographics (in=b); \\n    by CustomerID; \\n    if a and b; /* Ensure records exist in both datasets */ \\nrun;\",\n",
       " 'question': \"What child tables are created from the table 'SALES' ?\"}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_chain.invoke(\"What child tables are created from the table 'SALES' ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_chain = parallel_chain | prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What all will break if 'customer_orders_filtered' is deleted ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided code, deleting `customer_orders_filtered` will cause the `proc sql` step (Step 3) to fail because it cannot create the `customer_sales_summary` table. This failure will then cause the final data merge step (Step 4) to also fail, as it depends on the `customer_sales_summary` table.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = \"What will break if column 'quantity' is deleted from the table 'orders' ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the provided code, deleting the 'quantity' column from the 'orders' table will cause the first data step to fail. This step fails because it tries to calculate the `OrderValue` by multiplying `Quantity` and `UnitPrice`. This error will prevent the creation of `processed.customer_orders_raw` and cause all subsequent processing steps to fail as well.\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_chain.invoke(question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
